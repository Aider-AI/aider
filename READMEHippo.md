Here is the main program loop:

`main.py` calls `coder.run` to finish a task and loops back again only if receiving exception `SwitchCoder`  

<!--
    while True:
        try:
            coder.ok_to_warm_cache = bool(args.cache_keepalive_pings)
            coder.run()
            analytics.event("exit", reason="Completed main CLI coder.run")
            return
        except SwitchCoder as switch:
            coder.ok_to_warm_cache = False

            kwargs = dict(io=io, from_coder=coder)
            kwargs.update(switch.kwargs)
            if "show_announcements" in kwargs:
                del kwargs["show_announcements"]

            coder = Coder.create(**kwargs)

            if switch.kwargs.get("show_announcements") is not False:
                coder.show_announcements()
-->

Each coder subclass overloads necessary functions from `base_coder.py`. Important functions and properties include `gpt_prompts`, `get_edits`, `apply_edits`, and `run`, `run_one`.

`gpt_prompts` property defines LLM prompts                         
                                                                    
`get_edits` generating the edits that the coder wants to apply conditioned on current state and user input.

`apply_edits` function takes the list of edits generated by `get_edits` and applies them.                                   

The `run_one` function is similar to `run`, but it only runs once. It is useful for tasks that only need to be performed once, such as initializing the codebase or applying a one-time fix.  

The `run` function is the main entry point for the coder. At high level, `run` gets user prompts and relative file paths if mentioned, then it calls `run_one` to first fetch `url` and mentioned file list then have `llm` auto chat for some turns (`max_reflections`). The main operation of run_one is in `send_message` which parses input for file paths and add file contents to prompt, then sends request to llm`, and parse response to get mentioned files by llm to a list for the next send_message to read contents (reflection), get and execute shell commands, auto commit, run dry test.


Note that `run` with `with_message` param will be same as `run_one`. Otherwise, there is a run loop. 



`run` loops (`never end` until keyboard exception):
- copy context (in case of coder switch via `SwitchCoder` exception)
- ask for user input, and `fetch url` if available in inp
- `run_one`: (optionally) preproc user message,
    - then runs `send_message` up to `max_reflections` --- `self.reflected_message` is decided by `send_message`.
    - for user input, it will just end the run_once

- `send_message`: in run_once does most of the work
    - `format_messages` including adding file contents
        - call `format_chat_chunks` to assemble messages including: get repo, readyonly files, chat files messages to add file contents etc. to prompt (check `get_file_content`)
    - `send` message to LLM (after format and check token limit). 

        ```
        while True:
            try:
                yield from self.send(messages, functions=self.functions)
                break # only called when send finishes its yield loop
            except ... # Various error handlers
        ```

        
    - Response processing, including: 
        - handle function calls
        - process file mentions in `check_for_file_mentions` asking and taking input from user, and return a string like i added the files... to add to `self.reflected_message`
            - it'll then return from `send_message` to go back to the loop in `run_once` to repeat `send_message` with added file content in prompt.
        - `apply_edits` to apply changes to files mentioned in the LLM response        
        - modify `self.cur_messages`
    - auto-commit
    - run any shell commands metioned in the response
    - perform linting and testing if configured to do so.
    - State management: tracks message history, handle interruptions 


So currently, llm only auto loops when llm requests more files to be added via `self.reflected_message`. Otherwise, the coder will go back to `run` loop and wait for user input.
If we want multi-step reasoning agent, there are 2 ways to do:
    - make llm to response with continue or no, and extend `reflected_message` based on that.
    - or enable `run` loop to ask for agent input instead of user input.


*** New feature ***
## Agent auto loops to complete task
- Create a new base_code subclass, called `base_coder_auto_loop`
- Overload `run` to put limits on the while loop --- don't want to loop so long
- Overload `get_input` to instead of asking user for input, asking LLM for input of what next after `run_once` --- basically asking to continue or exit the loop, similar to cline asking if the task is complete.



# Add new coder
- create a new coder inheriting base_coder or another coder
- overwrite `edit_format` to something new --- class method in the root parent, `base_coder`, creates relevant coder instance based on this `edit_format` in most cases via `Commands._generic_chat_command`

- add the new coder in `__init__.py`

- add commandline command for the new coder: in `Commands`, create function `cmd_yourcodercommand`; the code search for function with that prefix `cmd` to get commands

- create coder prompt similarly


# Prompt
`format_chat_chunks` then `chunks.all_messages` is where aider get all prompts together, including system, readyonly_files, repo structure, done_messages (chat history or summary if history too long), chat files, current messages, reminder.

the chat can be restored from chat history file, look at `__init__` in `base_coder.py`

```
if not self.done_messages and restore_chat_history:
        history_md = self.io.read_text(self.io.chat_history_file)
```
 